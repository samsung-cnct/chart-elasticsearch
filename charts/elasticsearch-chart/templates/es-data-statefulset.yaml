apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: {{.Values.data_name}}
  labels:
    app: {{ template "name" . }}
    chart: {{ template "namewithversion" . }}
    release: "{{.Release.Name}}"
    heritage: "{{.Release.Service}}"
spec:
  serviceName: "{{.Values.data_name}}"
  replicas: {{.Values.data_replicas}}
  template:
    metadata:
      labels:
        app: {{.Values.name}}
        name: {{.Values.data_name}}
    spec:
      serviceAccount: {{.Values.name}}
      serviceAccountName: {{.Values.name}}
      initContainers:
      - name: sysctl
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext: {"privileged": true}
      - name: chown
        image: {{.Values.image}}
        command: ["chown", "-v", "elasticsearch:elasticsearch", {{ .Values.data_data_mount | quote }} ]
        volumeMounts:
        - name: esdata
          mountPath: {{ .Values.data_data_mount }}
      # end initContainers
      containers:
      - name: {{.Values.name}}
        image: {{.Values.image}}
        lifecycle:
          postStart:
            exec:
              # run hello.sh
              #                              Elasticsearch bin directory                                                     run file
              #                                             copy from read only filesystem       chmod in read-write filesystem
              command: [ "/bin/bash", "-c", "BIN=$(pwd)/bin;cp ${BIN}/hello.sh ${BIN}/hello2.sh; chmod 777 ${BIN}/hello2.sh; ${BIN}/hello2.sh" ]
          # end postStart
        ports:
        - name: {{ .Values.services.discovery.cluster_port_name }}
          containerPort: {{ .Values.services.discovery.cluster_port }}
        - name: {{ .Values.services.es.port_name }}
          containerPort: {{.Values.services.es.port}}
        resources:
          limits:
            cpu: {{.Values.data_cpu_limits}}
            memory: {{.Values.data_memory_limits}}
          requests:
            cpu: {{.Values.data_cpu_requests}}
            memory: {{.Values.data_memory_requests}}
        securityContext:
          privileged: true
          capabilities:
            add:
              - IPC_LOCK
              - SYS_RESOURCE
        env:
        {{- range $key, $value := .Values.data_envs }}
          - name: "{{ $key }}"
            value: "{{ $value }}"
        {{- end }}
          - name: ELASTIC_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: {{ template "fullname" . }}-auth
          - name: CLUSTER_NAME
            value: "{{ .Values.cluster_name }}"
          - name: SERVICE
            value: "{{ .Values.cluster_name }}"
          - name: NETWORK_HOST
            value: "{{ .Values.network_host }}"
          - name: DATA_MOUNT
            value: "{{ .Values.data_data_mount }}"
        #  the min/max *must* be the same due to elasticsearch 5.0 bootstrap checks.  For a production settings
        #  this should be 16GB.  Don't go over 32GB as the JVM starts to have issues.
          - name: ES_JAVA_OPTS
            value: "-Xms{{ .Values.data_Xms_Xmx }} -Xmx{{ .Values.data_Xms_Xmx }} -Djava.net.preferIPv4Stack=true"
          - name: KUBERNETES_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: DISCOVERY_SERVICE
            value: "{{ .Values.services.discovery.service_name }}"
          - name: "NAMESPACE"
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        imagePullPolicy: Always
      # end containers
      {{- if .Values.data_tolerations }}
      tolerations:
      {{- range .Values.data_tolerations }}
      - key: {{ .key | default ("") }}
        value: {{ .value | default ("")}}
        operator: {{ .operator | default ("Equal") }}
        effect: {{ .effect  | default ("")}}
      {{- end }}
      {{- end }}
      {{- if .Values.data_scheduling.affinity }}
      {{- if .Values.data_scheduling.affinity.node }}
      {{- if .Values.data_scheduling.affinity.node.labels }}
      affinity:
        nodeAffinity:
          {{ .Values.data_scheduling.affinity.node.type }}:
            nodeSelectorTerms:
            - matchExpressions:
                {{- range .Values.data_scheduling.affinity.node.labels }}
                - key: {{ .key }}
                  operator: {{ .operator }}
                  values:
                  {{- range .values }}
                  - {{ . }}
                  {{- end }}
                {{- end }}
      {{- end }}
      {{- end }}
      {{- end }}
        volumeMounts:
        - name: esconfig
          mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
          subPath: elasticsearch.yml
        - name: hellosh
          mountPath: /usr/share/elasticsearch/bin/hello.sh
          subPath: hello.sh
        - name: esdata
          mountPath: {{ .Values.data_data_mount }}
      volumes:
      - name: esconfig
        configMap:
          name: {{ template "name" . }}
      - name: hellosh
        configMap:
          name: {{ template "name" . }}
  volumeClaimTemplates:
  - metadata:
      name: esdata
      annotations:
              {{ .Values.data_storage_class_key }}: {{ .Values.data_storage_class_value }}
    spec:
      # unbound persistant volume claims if ReadWriteMany
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          #  set to small so when exploring with this the user doesn't accidentally create very large disks
          storage: {{.Values.data_volume_storage}}
